{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7183ebe8",
   "metadata": {},
   "source": [
    "# Entrenamiento y Evaluación de Modelos\n",
    "## Proyecto: Clasificación de Riesgo Crediticio\n",
    "\n",
    "### Objetivos de esta fase:\n",
    "1. **Selección de Características**: Aplicar técnicas de reducción dimensional\n",
    "2. **Entrenamiento de Modelos**: Implementar 3 algoritmos desde cero\n",
    "3. **Evaluación Exhaustiva**: Métricas completas y validación cruzada  \n",
    "4. **Optimización**: Ajuste de hiperparámetros para mejor rendimiento\n",
    "5. **Persistencia**: Guardar modelos entrenados para uso futuro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71acad",
   "metadata": {},
   "source": [
    "## 0. Setup y Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78087520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import json\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.insert(0, os.path.join(project_root, 'src'))\n",
    "\n",
    "from models import LogisticRegressionMulticlass, SVMMulticlass, RandomForestMulticlass\n",
    "from evaluation.metrics import (\n",
    "    ModelEvaluator, FeatureSelector, DimensionalityReducer, ModelPersistence,\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "print(\"Configuración completada exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b2568",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = os.path.join(project_root, 'data', 'processed')\n",
    "experiments_dir = os.path.join(project_root, 'experiments')\n",
    "os.makedirs(experiments_dir, exist_ok=True)\n",
    "\n",
    "print(\"CARGA DE DATOS PREPROCESADOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cargar datos\n",
    "X_train = pd.read_csv(os.path.join(processed_dir, 'X_train_processed.csv'))\n",
    "X_test = pd.read_csv(os.path.join(processed_dir, 'X_test_processed.csv'))\n",
    "y_train = pd.read_csv(os.path.join(processed_dir, 'y_train_processed.csv'))['nivel_riesgo_encoded'].values\n",
    "\n",
    "# Cargar metadatos\n",
    "with open(os.path.join(processed_dir, 'preprocessing_metadata.json'), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Cargar nombres de features\n",
    "with open(os.path.join(processed_dir, 'feature_names.txt'), 'r') as f:\n",
    "    feature_names = f.read().strip().split('\\n')\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"Clases: {metadata['target_classes']}\")\n",
    "print(f\"Distribución: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007c292",
   "metadata": {},
   "source": [
    "## 1. Análisis y Selección de Características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e16697",
   "metadata": {},
   "source": [
    "### 1.1 Análisis de Correlación entre Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANÁLISIS DE CORRELACIÓN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Filtrar features altamente correlacionadas\n",
    "features_to_keep_corr, features_to_remove_corr = FeatureSelector.correlation_filter(\n",
    "    X_train.values, threshold=0.95\n",
    ")\n",
    "\n",
    "print(f\"Features originales: {X_train.shape[1]}\")\n",
    "print(f\"Features a mantener (correlación): {len(features_to_keep_corr)}\")\n",
    "print(f\"Features removidas (correlación): {len(features_to_remove_corr)}\")\n",
    "\n",
    "if features_to_remove_corr:\n",
    "    print(\"\\nFeatures removidas por alta correlación:\")\n",
    "    for idx in features_to_remove_corr:\n",
    "        print(f\"  {feature_names[idx]}\")\n",
    "\n",
    "# Aplicar filtro de correlación\n",
    "X_train_corr = X_train.iloc[:, features_to_keep_corr]\n",
    "X_test_corr = X_test.iloc[:, features_to_keep_corr]\n",
    "feature_names_corr = [feature_names[i] for i in features_to_keep_corr]\n",
    "\n",
    "print(f\"\\nDimensiones después de filtro de correlación:\")\n",
    "print(f\"X_train: {X_train_corr.shape} | X_test: {X_test_corr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18069263",
   "metadata": {},
   "source": [
    "### 1.2 Selección Univariada con F-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SELECCIÓN UNIVARIADA (F-TEST)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Seleccionar top 20 features\n",
    "k_best = min(20, X_train_corr.shape[1])\n",
    "selected_features, f_scores = FeatureSelector.univariate_selection(\n",
    "    X_train_corr.values, y_train, k_best=k_best\n",
    ")\n",
    "\n",
    "print(f\"Top {k_best} features seleccionadas:\")\n",
    "feature_importance_pairs = [(feature_names_corr[i], f_scores[i]) for i in selected_features]\n",
    "feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (feature, score) in enumerate(feature_importance_pairs):\n",
    "    print(f\"{i+1:2d}. {feature[:35]:35} | F-score: {score:.3f}\")\n",
    "\n",
    "# Aplicar selección\n",
    "X_train_selected = X_train_corr.iloc[:, selected_features]\n",
    "X_test_selected = X_test_corr.iloc[:, selected_features]\n",
    "feature_names_selected = [feature_names_corr[i] for i in selected_features]\n",
    "\n",
    "print(f\"\\nDimensiones finales después de selección:\")\n",
    "print(f\"X_train: {X_train_selected.shape} | X_test: {X_test_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc964d13",
   "metadata": {},
   "source": [
    "### 1.3 Reducción Dimensional con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903aa286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANÁLISIS DE COMPONENTES PRINCIPALES (PCA)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Aplicar PCA manteniendo 95% de varianza\n",
    "pca = DimensionalityReducer(explained_variance_threshold=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_selected.values)\n",
    "X_test_pca = pca.transform(X_test_selected.values)\n",
    "\n",
    "print(f\"Componentes principales seleccionados: {pca.n_components_selected}\")\n",
    "print(f\"Varianza explicada acumulada: {np.sum(pca.explained_variance_ratio_):.3f}\")\n",
    "print(f\"Dimensiones PCA - Train: {X_train_pca.shape} | Test: {X_test_pca.shape}\")\n",
    "\n",
    "# Visualizar varianza explicada\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         pca.explained_variance_ratio_, 'bo-', markersize=6)\n",
    "plt.xlabel('Componente Principal')\n",
    "plt.ylabel('Varianza Explicada')\n",
    "plt.title('Varianza Explicada por Componente')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(cumsum_variance) + 1), cumsum_variance, 'ro-', markersize=6)\n",
    "plt.axhline(y=0.95, color='g', linestyle='--', alpha=0.7, label='95% varianza')\n",
    "plt.xlabel('Número de Componentes')\n",
    "plt.ylabel('Varianza Explicada Acumulada')\n",
    "plt.title('Varianza Explicada Acumulada')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPrimeras 5 componentes - varianza explicada:\")\n",
    "for i, var_exp in enumerate(pca.explained_variance_ratio_[:5]):\n",
    "    print(f\"  PC{i+1}: {var_exp:.4f} ({var_exp*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6ec82",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3d768",
   "metadata": {},
   "source": [
    "### 2.1 Configuración de Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211b218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CONFIGURACIÓN DE EXPERIMENTOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Preparar conjuntos de datos para experimentar\n",
    "datasets = {\n",
    "    'original': (X_train.values, X_test.values, feature_names),\n",
    "    'correlation_filtered': (X_train_corr.values, X_test_corr.values, feature_names_corr),\n",
    "    'feature_selected': (X_train_selected.values, X_test_selected.values, feature_names_selected),\n",
    "    'pca_transformed': (X_train_pca, X_test_pca, [f'PC{i+1}' for i in range(X_train_pca.shape[1])])\n",
    "}\n",
    "\n",
    "# Configurar modelos con diferentes hiperparámetros\n",
    "model_configs = {\n",
    "    'LogisticRegression': {\n",
    "        'class': LogisticRegressionMulticlass,\n",
    "        'params': {\n",
    "            'base': {'learning_rate': 0.01, 'max_iterations': 1000, 'regularization': 'l2', 'lambda_reg': 0.01},\n",
    "            'l1_reg': {'learning_rate': 0.01, 'max_iterations': 1000, 'regularization': 'l1', 'lambda_reg': 0.01},\n",
    "            'high_lr': {'learning_rate': 0.1, 'max_iterations': 500, 'regularization': 'l2', 'lambda_reg': 0.001}\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'class': SVMMulticlass,\n",
    "        'params': {\n",
    "            'base': {'C': 1.0, 'kernel': 'linear', 'learning_rate': 0.001, 'max_iterations': 1000},\n",
    "            'high_c': {'C': 10.0, 'kernel': 'linear', 'learning_rate': 0.001, 'max_iterations': 1000},\n",
    "            'rbf_kernel': {'C': 1.0, 'kernel': 'rbf', 'learning_rate': 0.001, 'max_iterations': 800}\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'class': RandomForestMulticlass,\n",
    "        'params': {\n",
    "            'base': {'n_estimators': 100, 'max_depth': 10, 'max_features': 'sqrt'},\n",
    "            'deep_trees': {'n_estimators': 50, 'max_depth': 20, 'max_features': 'sqrt'},\n",
    "            'many_trees': {'n_estimators': 200, 'max_depth': 8, 'max_features': 'log2'}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Datasets para experimentar: {list(datasets.keys())}\")\n",
    "print(f\"Modelos configurados: {list(model_configs.keys())}\")\n",
    "\n",
    "for dataset_name, (X_tr, X_te, features) in datasets.items():\n",
    "    print(f\"  {dataset_name}: {X_tr.shape[0]} × {X_tr.shape[1]} features\")\n",
    "\n",
    "# Inicializar almacenamiento de resultados\n",
    "experiment_results = {}\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006b31a",
   "metadata": {},
   "source": [
    "### 2.2 Entrenamiento y Evaluación Sistemática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ENTRENAMIENTO Y EVALUACIÓN DE MODELOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_experiments = sum(len(config['params']) for config in model_configs.values()) * len(datasets)\n",
    "experiment_count = 0\n",
    "\n",
    "for dataset_name, (X_train_exp, X_test_exp, feature_list) in datasets.items():\n",
    "    print(f\"\\nDATASET: {dataset_name.upper()} ({X_train_exp.shape[1]} features)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    dataset_results = {}\n",
    "    \n",
    "    for model_name, model_config in model_configs.items():\n",
    "        model_class = model_config['class']\n",
    "        \n",
    "        for param_name, params in model_config['params'].items():\n",
    "            experiment_count += 1\n",
    "            experiment_id = f\"{dataset_name}_{model_name}_{param_name}\"\n",
    "            \n",
    "            print(f\"\\n[{experiment_count}/{total_experiments}] {experiment_id}\")\n",
    "            print(f\"Parámetros: {params}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Crear y entrenar modelo\n",
    "                model = model_class(**params, random_state=42)\n",
    "                model.fit(X_train_exp, y_train)\n",
    "                \n",
    "                # Predicciones\n",
    "                y_train_pred = model.predict(X_train_exp)\n",
    "                \n",
    "                # Evaluación en entrenamiento\n",
    "                train_report = ModelEvaluator.classification_report(y_train, y_train_pred)\n",
    "                \n",
    "                # Validación cruzada\n",
    "                cv_results = ModelEvaluator.cross_validate(\n",
    "                    model, X_train_exp, y_train, cv=5, random_state=42\n",
    "                )\n",
    "                \n",
    "                training_time = time.time() - start_time\n",
    "                \n",
    "                # Guardar resultados\n",
    "                result = {\n",
    "                    'model': model,\n",
    "                    'dataset': dataset_name,\n",
    "                    'model_type': model_name,\n",
    "                    'params': params,\n",
    "                    'train_report': train_report,\n",
    "                    'cv_results': cv_results,\n",
    "                    'training_time': training_time,\n",
    "                    'n_features': X_train_exp.shape[1],\n",
    "                    'feature_names': feature_list\n",
    "                }\n",
    "                \n",
    "                dataset_results[experiment_id] = result\n",
    "                trained_models[experiment_id] = model\n",
    "                \n",
    "                # Mostrar métricas principales\n",
    "                print(f\"  Tiempo: {training_time:.2f}s\")\n",
    "                print(f\"  Train Accuracy: {train_report['accuracy']:.4f}\")\n",
    "                print(f\"  CV Accuracy: {cv_results['accuracy']['mean']:.4f} ± {cv_results['accuracy']['std']:.4f}\")\n",
    "                print(f\"  CV F1-Score: {cv_results['f1_macro']['mean']:.4f} ± {cv_results['f1_macro']['std']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    experiment_results[dataset_name] = dataset_results\n",
    "\n",
    "print(f\"\\n\\nEXPERIMENTOS COMPLETADOS: {len([r for ds in experiment_results.values() for r in ds.values()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257797db",
   "metadata": {},
   "source": [
    "## 3. Análisis de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47abb3",
   "metadata": {},
   "source": [
    "### 3.1 Comparación de Rendimiento por Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANÁLISIS COMPARATIVO DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear DataFrame con todos los resultados\n",
    "comparison_data = []\n",
    "\n",
    "for dataset_name, dataset_results in experiment_results.items():\n",
    "    for exp_id, result in dataset_results.items():\n",
    "        comparison_data.append({\n",
    "            'Experiment': exp_id,\n",
    "            'Dataset': dataset_name,\n",
    "            'Model': result['model_type'],\n",
    "            'Features': result['n_features'],\n",
    "            'Train_Accuracy': result['train_report']['accuracy'],\n",
    "            'CV_Accuracy_Mean': result['cv_results']['accuracy']['mean'],\n",
    "            'CV_Accuracy_Std': result['cv_results']['accuracy']['std'],\n",
    "            'CV_F1_Mean': result['cv_results']['f1_macro']['mean'],\n",
    "            'CV_F1_Std': result['cv_results']['f1_macro']['std'],\n",
    "            'CV_Precision_Mean': result['cv_results']['precision_macro']['mean'],\n",
    "            'CV_Recall_Mean': result['cv_results']['recall_macro']['mean'],\n",
    "            'Training_Time': result['training_time']\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Mostrar top 10 experimentos por F1-Score\n",
    "top_experiments = comparison_df.nlargest(10, 'CV_F1_Mean')\n",
    "\n",
    "print(\"TOP 10 EXPERIMENTOS POR F1-SCORE (Validación Cruzada):\")\n",
    "print(top_experiments[['Experiment', 'Dataset', 'Model', 'Features', 'CV_F1_Mean', 'CV_F1_Std', 'Training_Time']].to_string(index=False))\n",
    "\n",
    "# Análisis por modelo\n",
    "print(\"\\n\\nRESUMEN POR TIPO DE MODELO:\")\n",
    "model_summary = comparison_df.groupby('Model').agg({\n",
    "    'CV_F1_Mean': ['mean', 'max', 'std'],\n",
    "    'CV_Accuracy_Mean': ['mean', 'max', 'std'],\n",
    "    'Training_Time': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(model_summary)\n",
    "\n",
    "# Análisis por dataset\n",
    "print(\"\\n\\nRESUMEN POR DATASET:\")\n",
    "dataset_summary = comparison_df.groupby('Dataset').agg({\n",
    "    'CV_F1_Mean': ['mean', 'max', 'std'],\n",
    "    'CV_Accuracy_Mean': ['mean', 'max', 'std'],\n",
    "    'Features': 'first'\n",
    "}).round(4)\n",
    "\n",
    "print(dataset_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c8f72",
   "metadata": {},
   "source": [
    "### 3.2 Visualización de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18761e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones comparativas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. F1-Score por modelo y dataset\n",
    "pivot_f1 = comparison_df.pivot_table(\n",
    "    values='CV_F1_Mean', index='Dataset', columns='Model', aggfunc='max'\n",
    ")\n",
    "\n",
    "sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='viridis', ax=axes[0,0])\n",
    "axes[0,0].set_title('F1-Score Máximo por Modelo y Dataset')\n",
    "axes[0,0].set_xlabel('Modelo')\n",
    "axes[0,0].set_ylabel('Dataset')\n",
    "\n",
    "# 2. Accuracy vs F1-Score\n",
    "scatter = axes[0,1].scatter(comparison_df['CV_Accuracy_Mean'], comparison_df['CV_F1_Mean'], \n",
    "                           c=comparison_df['Training_Time'], cmap='plasma', alpha=0.7, s=60)\n",
    "axes[0,1].set_xlabel('CV Accuracy Mean')\n",
    "axes[0,1].set_ylabel('CV F1-Score Mean')\n",
    "axes[0,1].set_title('Accuracy vs F1-Score (Color: Tiempo de Entrenamiento)')\n",
    "plt.colorbar(scatter, ax=axes[0,1], label='Training Time (s)')\n",
    "\n",
    "# 3. Box plot por modelo\n",
    "comparison_df.boxplot(column='CV_F1_Mean', by='Model', ax=axes[1,0])\n",
    "axes[1,0].set_title('Distribución F1-Score por Modelo')\n",
    "axes[1,0].set_xlabel('Modelo')\n",
    "axes[1,0].set_ylabel('CV F1-Score Mean')\n",
    "\n",
    "# 4. Tiempo vs Rendimiento\n",
    "for model in comparison_df['Model'].unique():\n",
    "    model_data = comparison_df[comparison_df['Model'] == model]\n",
    "    axes[1,1].scatter(model_data['Training_Time'], model_data['CV_F1_Mean'], \n",
    "                     label=model, alpha=0.7, s=60)\n",
    "\n",
    "axes[1,1].set_xlabel('Training Time (s)')\n",
    "axes[1,1].set_ylabel('CV F1-Score Mean')\n",
    "axes[1,1].set_title('Tiempo de Entrenamiento vs Rendimiento')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f425c",
   "metadata": {},
   "source": [
    "### 3.3 Selección del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c64fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SELECCIÓN DEL MEJOR MODELO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Encontrar el mejor experimento basado en F1-Score\n",
    "best_experiment_idx = comparison_df['CV_F1_Mean'].idxmax()\n",
    "best_experiment = comparison_df.loc[best_experiment_idx]\n",
    "best_exp_id = best_experiment['Experiment']\n",
    "\n",
    "print(f\"MEJOR EXPERIMENTO: {best_exp_id}\")\n",
    "print(f\"  Modelo: {best_experiment['Model']}\")\n",
    "print(f\"  Dataset: {best_experiment['Dataset']}\")\n",
    "print(f\"  Features: {best_experiment['Features']}\")\n",
    "print(f\"  CV F1-Score: {best_experiment['CV_F1_Mean']:.4f} ± {best_experiment['CV_F1_Std']:.4f}\")\n",
    "print(f\"  CV Accuracy: {best_experiment['CV_Accuracy_Mean']:.4f} ± {best_experiment['CV_Accuracy_Std']:.4f}\")\n",
    "print(f\"  Tiempo de entrenamiento: {best_experiment['Training_Time']:.2f}s\")\n",
    "\n",
    "# Obtener el modelo y resultados completos\n",
    "dataset_name = best_experiment['Dataset']\n",
    "best_result = experiment_results[dataset_name][best_exp_id]\n",
    "best_model = trained_models[best_exp_id]\n",
    "\n",
    "print(\"\\nRESULTADOS DETALLADOS DEL MEJOR MODELO:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Mostrar matriz de confusión de validación cruzada promedio\n",
    "train_report = best_result['train_report']\n",
    "print(\"\\nMatriz de Confusión (Entrenamiento):\")\n",
    "print(train_report['confusion_matrix'])\n",
    "\n",
    "print(\"\\nMétricas por Clase:\")\n",
    "classes = train_report['classes']\n",
    "class_names = ['Alto', 'Bajo', 'Medio']  # Mapear índices a nombres\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    precision = train_report['per_class']['precision'][i]\n",
    "    recall = train_report['per_class']['recall'][i]\n",
    "    f1 = train_report['per_class']['f1_score'][i]\n",
    "    support = train_report['per_class']['support'][i]\n",
    "    \n",
    "    print(f\"  {class_name:8} | Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f} | Support: {support}\")\n",
    "\n",
    "print(f\"\\nMétricas Globales (Entrenamiento):\")\n",
    "print(f\"  Accuracy: {train_report['accuracy']:.4f}\")\n",
    "print(f\"  Macro Avg - Precision: {train_report['macro_avg']['precision']:.4f}\")\n",
    "print(f\"  Macro Avg - Recall: {train_report['macro_avg']['recall']:.4f}\")\n",
    "print(f\"  Macro Avg - F1: {train_report['macro_avg']['f1_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nMétricas de Validación Cruzada:\")\n",
    "cv_results = best_result['cv_results']\n",
    "for metric, stats in cv_results.items():\n",
    "    print(f\"  {metric}: {stats['mean']:.4f} ± {stats['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780e9c0",
   "metadata": {},
   "source": [
    "## 4. Análisis de Importancia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6264a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANÁLISIS DE IMPORTANCIA DE FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Obtener importancia de features del mejor modelo\n",
    "if hasattr(best_model, 'get_feature_importance'):\n",
    "    feature_importance = best_model.get_feature_importance()\n",
    "    feature_names_best = best_result['feature_names']\n",
    "    \n",
    "    # Crear DataFrame para mejor visualización\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names_best,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"TOP 15 FEATURES MÁS IMPORTANTES ({best_experiment['Model']}):\")\n",
    "    print(importance_df.head(15).to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Visualizar importancia\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = importance_df.head(15)\n",
    "    \n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), \n",
    "               [f[:30] + ('...' if len(f) > 30 else '') for f in top_features['Feature']])\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.title(f'Top 15 Features - {best_experiment[\"Model\"]} (Dataset: {best_experiment[\"Dataset\"]})')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Guardar importancia\n",
    "    importance_df.to_csv(\n",
    "        os.path.join(experiments_dir, f'feature_importance_{best_exp_id}.csv'), \n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(f\"El modelo {best_experiment['Model']} no provee análisis de importancia de features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da04f08",
   "metadata": {},
   "source": [
    "## 5. Guardado de Modelos y Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca6bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GUARDADO DE MODELOS Y RESULTADOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Crear directorio de modelos\n",
    "models_dir = os.path.join(experiments_dir, 'trained_models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "best_model_path = os.path.join(models_dir, f'best_model_{best_exp_id}.pkl')\n",
    "best_model_metadata = {\n",
    "    'experiment_id': best_exp_id,\n",
    "    'model_type': best_experiment['Model'],\n",
    "    'dataset_used': best_experiment['Dataset'],\n",
    "    'n_features': best_experiment['Features'],\n",
    "    'cv_f1_score': best_experiment['CV_F1_Mean'],\n",
    "    'cv_accuracy': best_experiment['CV_Accuracy_Mean'],\n",
    "    'feature_names': best_result['feature_names'],\n",
    "    'target_classes': metadata['target_classes'],\n",
    "    'target_encoding': metadata['target_encoding'],\n",
    "    'training_time': best_experiment['Training_Time']\n",
    "}\n",
    "\n",
    "ModelPersistence.save_model(best_model, best_model_path, best_model_metadata)\n",
    "print(f\"Mejor modelo guardado: {best_model_path}\")\n",
    "\n",
    "# Guardar top 5 modelos\n",
    "top_5_experiments = comparison_df.nlargest(5, 'CV_F1_Mean')\n",
    "\n",
    "for idx, row in top_5_experiments.iterrows():\n",
    "    exp_id = row['Experiment']\n",
    "    model = trained_models[exp_id]\n",
    "    \n",
    "    model_path = os.path.join(models_dir, f'model_{exp_id}.pkl')\n",
    "    model_metadata = {\n",
    "        'experiment_id': exp_id,\n",
    "        'model_type': row['Model'],\n",
    "        'dataset_used': row['Dataset'],\n",
    "        'cv_f1_score': row['CV_F1_Mean'],\n",
    "        'cv_accuracy': row['CV_Accuracy_Mean'],\n",
    "        'rank': len(top_5_experiments) - list(top_5_experiments.index).index(idx)\n",
    "    }\n",
    "    \n",
    "    ModelPersistence.save_model(model, model_path, model_metadata)\n",
    "\n",
    "print(f\"Top 5 modelos guardados en: {models_dir}\")\n",
    "\n",
    "# Guardar resultados completos\n",
    "results_path = os.path.join(experiments_dir, 'experiment_results.pkl')\n",
    "ModelPersistence.save_results({\n",
    "    'experiment_results': experiment_results,\n",
    "    'comparison_df': comparison_df,\n",
    "    'best_experiment_id': best_exp_id,\n",
    "    'model_configs': model_configs,\n",
    "    'datasets_info': {name: {'shape': data[0].shape, 'features': len(data[2])} \n",
    "                     for name, data in datasets.items()}\n",
    "}, results_path)\n",
    "\n",
    "print(f\"Resultados completos guardados: {results_path}\")\n",
    "\n",
    "# Guardar resumen en CSV\n",
    "comparison_df.to_csv(os.path.join(experiments_dir, 'model_comparison.csv'), index=False)\n",
    "print(f\"Comparación de modelos: {os.path.join(experiments_dir, 'model_comparison.csv')}\")\n",
    "\n",
    "print(\"\\nARCHIVOS GENERADOS:\")\n",
    "for root, dirs, files in os.walk(experiments_dir):\n",
    "    level = root.replace(experiments_dir, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abf0e5",
   "metadata": {},
   "source": [
    "## 6. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae310ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESUMEN FINAL DEL ENTRENAMIENTO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"EXPERIMENTOS REALIZADOS:\n",
    "- Total de configuraciones probadas: {len(comparison_df)}\n",
    "- Datasets evaluados: {len(datasets)}\n",
    "- Algoritmos implementados: {len(model_configs)}\n",
    "- Técnicas de selección de features aplicadas: 3 (Correlación, F-test, PCA)\n",
    "\n",
    "MEJOR CONFIGURACIÓN:\n",
    "- Experimento: {best_exp_id}\n",
    "- Modelo: {best_experiment['Model']}\n",
    "- Dataset: {best_experiment['Dataset']}\n",
    "- Features utilizadas: {best_experiment['Features']}\n",
    "- F1-Score (CV): {best_experiment['CV_F1_Mean']:.4f} ± {best_experiment['CV_F1_Std']:.4f}\n",
    "- Accuracy (CV): {best_experiment['CV_Accuracy_Mean']:.4f} ± {best_experiment['CV_Accuracy_Std']:.4f}\n",
    "- Tiempo de entrenamiento: {best_experiment['Training_Time']:.2f} segundos\n",
    "\n",
    "COMPARACIÓN DE ALGORITMOS (Mejor F1-Score):\n",
    "\"\"\")\n",
    "\n",
    "# Resumen por algoritmo\n",
    "algo_best = comparison_df.loc[comparison_df.groupby('Model')['CV_F1_Mean'].idxmax()]\n",
    "for _, row in algo_best.iterrows():\n",
    "    print(f\"- {row['Model']:15}: F1={row['CV_F1_Mean']:.4f}, Acc={row['CV_Accuracy_Mean']:.4f}, Time={row['Training_Time']:.1f}s\")\n",
    "\n",
    "print(f\"\"\"\\nEFECTO DE SELECCIÓN DE FEATURES:\n",
    "\"\"\")\n",
    "\n",
    "# Resumen por dataset\n",
    "dataset_best = comparison_df.loc[comparison_df.groupby('Dataset')['CV_F1_Mean'].idxmax()]\n",
    "for _, row in dataset_best.iterrows():\n",
    "    print(f\"- {row['Dataset']:20}: {row['Features']:2d} features, F1={row['CV_F1_Mean']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\\nOPTIMIZACIÓN LOGRADA:\n",
    "- Mejora en F1-Score vs baseline: {(comparison_df['CV_F1_Mean'].max() - comparison_df['CV_F1_Mean'].min()):.4f}\n",
    "- Reducción de dimensionalidad máxima: {X_train.shape[1] - comparison_df['Features'].min()} features\n",
    "- Modelos entrenados y listos para producción: {len(top_5_experiments)}\n",
    "\n",
    "ARCHIVOS GENERADOS:\n",
    "- Mejor modelo: {best_model_path}\n",
    "- Comparación completa: {os.path.join(experiments_dir, 'model_comparison.csv')}\n",
    "- Resultados detallados: {results_path}\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FASE DE MODELADO COMPLETADA EXITOSAMENTE\")\n",
    "print(\"Siguiente paso: Evaluación final en datos de test\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
