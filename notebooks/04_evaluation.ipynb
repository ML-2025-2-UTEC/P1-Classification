{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a5f524",
   "metadata": {},
   "source": [
    "# Evaluación Final y Análisis de Resultados\n",
    "## Proyecto: Clasificación de Riesgo Crediticio\n",
    "\n",
    "### Objetivos de esta fase:\n",
    "1. **Evaluación Final**: Predicciones en conjunto de test con el mejor modelo\n",
    "2. **Análisis de Interpretabilidad**: Features más importantes y casos de estudio\n",
    "3. **Métricas de Negocio**: Análisis del costo de errores y ROI\n",
    "4. **Recomendaciones**: Estrategias para implementación en producción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0589644",
   "metadata": {},
   "source": [
    "## 0. Setup y Carga de Resultados Previos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ca427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.insert(0, os.path.join(project_root, 'src'))\n",
    "\n",
    "from evaluation.metrics import (\n",
    "    ModelEvaluator, ModelPersistence, \n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "print(\"Configuración completada exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios de trabajo\n",
    "processed_dir = os.path.join(project_root, 'data', 'processed')\n",
    "experiments_dir = os.path.join(project_root, 'experiments')\n",
    "models_dir = os.path.join(experiments_dir, 'trained_models')\n",
    "reports_dir = os.path.join(project_root, 'reports')\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "print(\"CARGA DE DATOS Y RESULTADOS PREVIOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cargar datos de test\n",
    "X_test = pd.read_csv(os.path.join(processed_dir, 'X_test_processed.csv'))\n",
    "print(f\"Conjunto de test cargado: {X_test.shape}\")\n",
    "\n",
    "# Cargar metadatos\n",
    "with open(os.path.join(processed_dir, 'preprocessing_metadata.json'), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"Clases objetivo: {metadata['target_classes']}\")\n",
    "print(f\"Encoding: {metadata['target_encoding']}\")\n",
    "\n",
    "# Verificar si existen resultados de entrenamiento\n",
    "results_path = os.path.join(experiments_dir, 'experiment_results.pkl')\n",
    "comparison_path = os.path.join(experiments_dir, 'model_comparison.csv')\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    experiment_results = ModelPersistence.load_results(results_path)\n",
    "    print(\"\\nResultados de entrenamiento encontrados\")\n",
    "else:\n",
    "    print(\"\\nResultados de entrenamiento no encontrados\")\n",
    "    print(\"Ejecute primero el notebook 03_modeling.ipynb\")\n",
    "\n",
    "if os.path.exists(comparison_path):\n",
    "    comparison_df = pd.read_csv(comparison_path)\n",
    "    print(f\"Comparación de modelos cargada: {len(comparison_df)} experimentos\")\n",
    "else:\n",
    "    print(\"Archivo de comparación no encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9cdc4f",
   "metadata": {},
   "source": [
    "## 1. Identificación del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349961d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'comparison_df' in locals():\n",
    "    print(\"IDENTIFICACIÓN DEL MEJOR MODELO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Encontrar mejor modelo por F1-Score\n",
    "    best_idx = comparison_df['CV_F1_Mean'].idxmax()\n",
    "    best_experiment = comparison_df.loc[best_idx]\n",
    "    best_exp_id = best_experiment['Experiment']\n",
    "    \n",
    "    print(f\"MEJOR MODELO IDENTIFICADO: {best_exp_id}\")\n",
    "    print(f\"  Tipo: {best_experiment['Model']}\")\n",
    "    print(f\"  Dataset: {best_experiment['Dataset']}\")\n",
    "    print(f\"  Features: {best_experiment['Features']}\")\n",
    "    print(f\"  CV F1-Score: {best_experiment['CV_F1_Mean']:.4f} ± {best_experiment['CV_F1_Std']:.4f}\")\n",
    "    print(f\"  CV Accuracy: {best_experiment['CV_Accuracy_Mean']:.4f} ± {best_experiment['CV_Accuracy_Std']:.4f}\")\n",
    "    \n",
    "    # Buscar archivo del modelo\n",
    "    best_model_path = os.path.join(models_dir, f'best_model_{best_exp_id}.pkl')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f\"\\nModelo encontrado: {best_model_path}\")\n",
    "        best_model, model_metadata = ModelPersistence.load_model(best_model_path)\n",
    "        print(f\"Modelo cargado exitosamente\")\n",
    "        print(f\"Metadatos: {model_metadata['model_type']} con {model_metadata['n_features']} features\")\n",
    "    else:\n",
    "        print(f\"\\nArchivo del modelo no encontrado: {best_model_path}\")\n",
    "        print(\"Buscar modelos alternativos...\")\n",
    "        \n",
    "        # Buscar otros modelos disponibles\n",
    "        available_models = [f for f in os.listdir(models_dir) if f.endswith('.pkl')]\n",
    "        if available_models:\n",
    "            print(f\"Modelos disponibles: {available_models}\")\n",
    "            # Cargar el primero disponible como fallback\n",
    "            fallback_path = os.path.join(models_dir, available_models[0])\n",
    "            best_model, model_metadata = ModelPersistence.load_model(fallback_path)\n",
    "            print(f\"Modelo de respaldo cargado: {available_models[0]}\")\n",
    "        else:\n",
    "            print(\"No se encontraron modelos entrenados\")\n",
    "            print(\"Ejecute primero el notebook de entrenamiento\")\n",
    "else:\n",
    "    print(\"Datos de comparación no disponibles\")\n",
    "    print(\"Ejecute primero el notebook 03_modeling.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5878c3a",
   "metadata": {},
   "source": [
    "## 2. Preparación de Datos para Evaluación Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73369530",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'best_model' in locals() and 'model_metadata' in locals():\n",
    "    print(\"PREPARACIÓN DE DATOS PARA EVALUACIÓN FINAL\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Determinar qué features usar según el dataset del mejor modelo\n",
    "    dataset_used = model_metadata.get('dataset_used', 'original')\n",
    "    feature_names_used = model_metadata.get('feature_names', X_test.columns.tolist())\n",
    "    \n",
    "    print(f\"Dataset utilizado por el mejor modelo: {dataset_used}\")\n",
    "    print(f\"Features esperadas: {len(feature_names_used)}\")\n",
    "    \n",
    "    # Preparar datos de test según el dataset usado\n",
    "    if dataset_used == 'original':\n",
    "        X_test_final = X_test.values\n",
    "        feature_names_final = X_test.columns.tolist()\n",
    "        \n",
    "    elif dataset_used == 'correlation_filtered':\n",
    "        # Aplicar filtro de correlación\n",
    "        from evaluation.metrics import FeatureSelector\n",
    "        features_to_keep, _ = FeatureSelector.correlation_filter(X_test.values, threshold=0.95)\n",
    "        X_test_final = X_test.iloc[:, features_to_keep].values\n",
    "        feature_names_final = [X_test.columns[i] for i in features_to_keep]\n",
    "        \n",
    "    elif dataset_used == 'feature_selected':\n",
    "        # Aplicar selección de features (usar las mismas features que en entrenamiento)\n",
    "        available_features = X_test.columns.tolist()\n",
    "        selected_indices = [available_features.index(fname) for fname in feature_names_used if fname in available_features]\n",
    "        X_test_final = X_test.iloc[:, selected_indices].values\n",
    "        feature_names_final = [available_features[i] for i in selected_indices]\n",
    "        \n",
    "    elif dataset_used == 'pca_transformed':\n",
    "        print(\"Dataset PCA detectado - requiere transformación\")\n",
    "        print(\"Para PCA, necesitaríamos el objeto transformador original\")\n",
    "        # Por simplicidad, usar features seleccionadas\n",
    "        X_test_final = X_test.iloc[:, :model_metadata['n_features']].values\n",
    "        feature_names_final = X_test.columns[:model_metadata['n_features']].tolist()\n",
    "    else:\n",
    "        # Fallback: usar todas las features disponibles\n",
    "        X_test_final = X_test.values\n",
    "        feature_names_final = X_test.columns.tolist()\n",
    "    \n",
    "    print(f\"\\nDatos de test preparados:\")\n",
    "    print(f\"  Forma: {X_test_final.shape}\")\n",
    "    print(f\"  Features: {len(feature_names_final)}\")\n",
    "    \n",
    "    # Verificar compatibilidad\n",
    "    expected_features = model_metadata.get('n_features', X_test_final.shape[1])\n",
    "    if X_test_final.shape[1] != expected_features:\n",
    "        print(f\"\\nAdvertencia: Dimensiones no coinciden\")\n",
    "        print(f\"     Esperado: {expected_features}, Actual: {X_test_final.shape[1]}\")\n",
    "        \n",
    "        # Ajustar dimensiones\n",
    "        if X_test_final.shape[1] > expected_features:\n",
    "            X_test_final = X_test_final[:, :expected_features]\n",
    "            print(f\"     Ajustado a {X_test_final.shape[1]} features\")\n",
    "        else:\n",
    "            print(f\"     No se puede ajustar - faltan features\")\n",
    "\n",
    "else:\n",
    "    print(\"Modelo no disponible para evaluación\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c44b3",
   "metadata": {},
   "source": [
    "## 3. Predicciones Finales en Conjunto de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'best_model' in locals() and 'X_test_final' in locals():\n",
    "    print(\"PREDICCIONES FINALES EN CONJUNTO DE TEST\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Realizar predicciones\n",
    "        print(\"Generando predicciones...\")\n",
    "        y_test_pred = best_model.predict(X_test_final)\n",
    "        \n",
    "        # Obtener probabilidades si están disponibles\n",
    "        try:\n",
    "            y_test_proba = best_model.predict_proba(X_test_final)\n",
    "            print(\"Probabilidades de predicción obtenidas\")\n",
    "        except:\n",
    "            y_test_proba = None\n",
    "            print(\"Probabilidades no disponibles para este modelo\")\n",
    "        \n",
    "        print(f\"\\nPredicciones completadas:\")\n",
    "        print(f\"  Muestras procesadas: {len(y_test_pred)}\")\n",
    "        print(f\"  Clases únicas predichas: {np.unique(y_test_pred)}\")\n",
    "        \n",
    "        # Distribución de predicciones\n",
    "        pred_counts = np.bincount(y_test_pred)\n",
    "        target_classes = metadata['target_classes']\n",
    "        \n",
    "        print(f\"\\nDistribución de Predicciones:\")\n",
    "        for i, class_name in enumerate(target_classes):\n",
    "            if i < len(pred_counts):\n",
    "                count = pred_counts[i]\n",
    "                percentage = (count / len(y_test_pred)) * 100\n",
    "                print(f\"  {class_name:8}: {count:5d} ({percentage:5.1f}%)\")\n",
    "        \n",
    "        # Crear DataFrame con resultados\n",
    "        results_df = pd.DataFrame({\n",
    "            'Prediction': y_test_pred,\n",
    "            'Risk_Level': [target_classes[pred] for pred in y_test_pred]\n",
    "        })\n",
    "        \n",
    "        if y_test_proba is not None:\n",
    "            for i, class_name in enumerate(target_classes):\n",
    "                if i < y_test_proba.shape[1]:\n",
    "                    results_df[f'Prob_{class_name}'] = y_test_proba[:, i]\n",
    "        \n",
    "        print(f\"\\nResultados organizados en DataFrame: {results_df.shape}\")\n",
    "        print(results_df.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar predicciones: {str(e)}\")\n",
    "        print(\"Verifique la compatibilidad del modelo con los datos\")\n",
    "\n",
    "else:\n",
    "    print(\"No se puede realizar predicciones - modelo o datos no disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae28c81",
   "metadata": {},
   "source": [
    "## 4. Análisis de Interpretabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55dade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'best_model' in locals():\n",
    "    print(\"ANÁLISIS DE INTERPRETABILIDAD DEL MODELO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Obtener importancia de features si está disponible\n",
    "    if hasattr(best_model, 'get_feature_importance'):\n",
    "        try:\n",
    "            feature_importance = best_model.get_feature_importance()\n",
    "            \n",
    "            # Crear DataFrame de importancia\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names_final[:len(feature_importance)],\n",
    "                'Importance': feature_importance\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nTOP 15 FEATURES MÁS IMPORTANTES:\")\n",
    "            print(\"-\" * 60)\n",
    "            top_features = importance_df.head(15)\n",
    "            \n",
    "            for idx, row in top_features.iterrows():\n",
    "                feature_name = row['Feature'][:40]  # Truncar nombre largo\n",
    "                importance = row['Importance']\n",
    "                print(f\"{idx+1:2d}. {feature_name:40} | {importance:.6f}\")\n",
    "            \n",
    "            # Visualización de importancia\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_20 = importance_df.head(20)\n",
    "            \n",
    "            plt.barh(range(len(top_20)), top_20['Importance'])\n",
    "            plt.yticks(range(len(top_20)), \n",
    "                      [name[:35] + '...' if len(name) > 35 else name for name in top_20['Feature']])\n",
    "            plt.xlabel('Importancia Relativa')\n",
    "            plt.title(f'Top 20 Features Más Importantes - {model_metadata[\"model_type\"]}')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.grid(True, alpha=0.3, axis='x')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Guardar importancia\n",
    "            importance_path = os.path.join(reports_dir, 'feature_importance_final.csv')\n",
    "            importance_df.to_csv(importance_path, index=False)\n",
    "            print(f\"\\nImportancia guardada en: {importance_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error al obtener importancia de features: {str(e)}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"El modelo {model_metadata['model_type']} no provee análisis de importancia\")\n",
    "    \n",
    "    # Análisis específico por tipo de modelo\n",
    "    model_type = model_metadata.get('model_type', 'Unknown')\n",
    "    \n",
    "    if 'LogisticRegression' in model_type:\n",
    "        print(f\"\\nANÁLISIS ESPECÍFICO - REGRESIÓN LOGÍSTICA:\")\n",
    "        if hasattr(best_model, 'weights') and hasattr(best_model, 'bias'):\n",
    "            print(f\"  Dimensiones de pesos: {best_model.weights.shape}\")\n",
    "            print(f\"  Bias: {best_model.bias}\")\n",
    "            print(f\"  Regularización aplicada: {getattr(best_model, 'regularization', 'N/A')}\")\n",
    "            \n",
    "            # Mostrar pesos más significativos por clase\n",
    "            if len(best_model.weights.shape) == 2:\n",
    "                for class_idx, class_name in enumerate(target_classes):\n",
    "                    if class_idx < best_model.weights.shape[1]:\n",
    "                        class_weights = best_model.weights[:, class_idx]\n",
    "                        top_positive = np.argsort(class_weights)[-5:]\n",
    "                        top_negative = np.argsort(class_weights)[:5]\n",
    "                        \n",
    "                        print(f\"\\n  Clase {class_name}:\")\n",
    "                        print(f\"    Top features positivos (aumentan probabilidad):\")\n",
    "                        for idx in reversed(top_positive):\n",
    "                            if idx < len(feature_names_final):\n",
    "                                print(f\"      {feature_names_final[idx][:30]:30}: {class_weights[idx]:+.4f}\")\n",
    "                        \n",
    "                        print(f\"    Top features negativos (disminuyen probabilidad):\")\n",
    "                        for idx in top_negative:\n",
    "                            if idx < len(feature_names_final):\n",
    "                                print(f\"      {feature_names_final[idx][:30]:30}: {class_weights[idx]:+.4f}\")\n",
    "    \n",
    "    elif 'RandomForest' in model_type:\n",
    "        print(f\"\\nANÁLISIS ESPECÍFICO - RANDOM FOREST:\")\n",
    "        if hasattr(best_model, 'trees'):\n",
    "            print(f\"  Número de árboles: {len(best_model.trees)}\")\n",
    "            print(f\"  Profundidad máxima configurada: {getattr(best_model, 'max_depth', 'N/A')}\")\n",
    "            print(f\"  Features por árbol: {getattr(best_model, 'max_features', 'N/A')}\")\n",
    "    \n",
    "    elif 'SVM' in model_type:\n",
    "        print(f\"\\nANÁLISIS ESPECÍFICO - SVM:\")\n",
    "        if hasattr(best_model, 'binary_classifiers'):\n",
    "            print(f\"  Clasificadores binarios: {len(best_model.binary_classifiers)}\")\n",
    "            print(f\"  Kernel utilizado: {getattr(best_model, 'kernel', 'N/A')}\")\n",
    "            print(f\"  Parámetro C: {getattr(best_model, 'C', 'N/A')}\")\n",
    "\n",
    "else:\n",
    "    print(\"Modelo no disponible para análisis de interpretabilidad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e41c2",
   "metadata": {},
   "source": [
    "## 5. Análisis de Casos de Estudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' in locals() and 'X_test_final' in locals():\n",
    "    print(\"ANÁLISIS DE CASOS DE ESTUDIO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Seleccionar casos representativos de cada clase\n",
    "    sample_cases = {}\n",
    "    \n",
    "    for class_idx, class_name in enumerate(target_classes):\n",
    "        class_predictions = results_df[results_df['Prediction'] == class_idx]\n",
    "        \n",
    "        if len(class_predictions) > 0:\n",
    "            # Tomar casos con mayor confianza (si disponible)\n",
    "            if f'Prob_{class_name}' in results_df.columns:\n",
    "                # Ordenar por probabilidad de la clase predicha\n",
    "                class_predictions_sorted = class_predictions.sort_values(\n",
    "                    f'Prob_{class_name}', ascending=False\n",
    "                )\n",
    "                sample_indices = class_predictions_sorted.index[:3].tolist()  # Top 3\n",
    "            else:\n",
    "                # Tomar muestras aleatorias\n",
    "                sample_indices = class_predictions.index[:3].tolist()\n",
    "            \n",
    "            sample_cases[class_name] = sample_indices\n",
    "    \n",
    "    # Mostrar casos de estudio\n",
    "    for class_name, indices in sample_cases.items():\n",
    "        print(f\"\\nCASOS DE ESTUDIO - RIESGO {class_name.upper()}:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            print(f\"\\n  Caso {i+1} (Muestra #{idx}):\")\n",
    "            \n",
    "            # Mostrar probabilidades si están disponibles\n",
    "            if y_test_proba is not None:\n",
    "                print(f\"    Probabilidades:\")\n",
    "                for j, cls_name in enumerate(target_classes):\n",
    "                    if j < y_test_proba.shape[1]:\n",
    "                        prob = y_test_proba[idx, j]\n",
    "                        print(f\"      {cls_name}: {prob:.3f}\")\n",
    "            \n",
    "            # Mostrar features más relevantes para este caso\n",
    "            if 'importance_df' in locals() and len(feature_names_final) == X_test_final.shape[1]:\n",
    "                case_features = X_test_final[idx]\n",
    "                top_features_idx = importance_df.head(10).index.tolist()\n",
    "                \n",
    "                print(f\"    Top 5 features más importantes:\")\n",
    "                for feat_idx in top_features_idx[:5]:\n",
    "                    if feat_idx < len(feature_names_final) and feat_idx < len(case_features):\n",
    "                        feature_name = feature_names_final[feat_idx]\n",
    "                        feature_value = case_features[feat_idx]\n",
    "                        importance = importance_df.loc[feat_idx, 'Importance']\n",
    "                        print(f\"      {feature_name[:25]:25}: {feature_value:8.3f} (imp: {importance:.4f})\")\n",
    "    \n",
    "    print(f\"\\nAnálisis de casos completado\")\n",
    "\n",
    "else:\n",
    "    print(\"Datos de predicciones no disponibles para análisis de casos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8b3ad",
   "metadata": {},
   "source": [
    "## 6. Métricas de Negocio y Análisis de Costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c45b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' in locals():\n",
    "    print(\"ANÁLISIS DE MÉTRICAS DE NEGOCIO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Definir costos de clasificación errónea (ejemplo)\n",
    "    # Matriz de costos: [Verdadero, Predicho] -> Costo\n",
    "    cost_matrix = np.array([\n",
    "        [0,    50,   200],  # Alto real: correcto, pred Bajo (muy malo), pred Medio (malo)\n",
    "        [10,   0,    30],   # Bajo real: pred Alto (conservador), correcto, pred Medio (aceptable)  \n",
    "        [25,   15,   0]     # Medio real: pred Alto (conservador), pred Bajo (riesgoso), correcto\n",
    "    ])\n",
    "    \n",
    "    class_names = ['Alto', 'Bajo', 'Medio']\n",
    "    \n",
    "    print(\"MATRIZ DE COSTOS DEFINIDA (por error):\")\n",
    "    print(\"\\nReal \\\\ Predicho    Alto    Bajo    Medio\")\n",
    "    for i, true_class in enumerate(class_names):\n",
    "        print(f\"{true_class:12}\", end=\"\")\n",
    "        for j in range(len(class_names)):\n",
    "            print(f\"{cost_matrix[i,j]:8}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Simular etiquetas verdaderas para análisis (en producción se tendrían)\n",
    "    # Generar distribución realista basada en las predicciones\n",
    "    np.random.seed(42)\n",
    "    n_samples = len(results_df)\n",
    "    \n",
    "    # Distribución aproximada: 30% Alto, 40% Medio, 30% Bajo\n",
    "    true_distribution = [0.3, 0.3, 0.4]  # Alto, Bajo, Medio\n",
    "    y_true_simulated = np.random.choice(3, n_samples, p=true_distribution)\n",
    "    \n",
    "    print(f\"\\n\\nANÁLISIS CON ETIQUETAS SIMULADAS:\")\n",
    "    print(f\"(En producción se usarían etiquetas reales)\")\n",
    "    \n",
    "    # Calcular matriz de confusión\n",
    "    cm = confusion_matrix(y_true_simulated, results_df['Prediction'].values)\n",
    "    \n",
    "    print(f\"\\nMatriz de Confusión:\")\n",
    "    print(\"\\nReal \\\\ Pred     Alto    Bajo    Medio   Total\")\n",
    "    class_names = ['Alto', 'Bajo', 'Medio']\n",
    "    for i, true_class in enumerate(class_names):\n",
    "        row_sum = np.sum(cm[i, :])\n",
    "        print(f\"{true_class:12}\", end=\"\")\n",
    "        for j in range(len(class_names)):\n",
    "            print(f\"{cm[i,j]:8}\", end=\"\")\n",
    "        print(f\"{row_sum:8}\")\n",
    "    \n",
    "    col_sums = np.sum(cm, axis=0)\n",
    "    print(f\"{'Total':12}\", end=\"\")\n",
    "    for col_sum in col_sums:\n",
    "        print(f\"{col_sum:8}\", end=\"\")\n",
    "    print(f\"{np.sum(cm):8}\")\n",
    "    \n",
    "    # Calcular costo total\n",
    "    total_cost = 0\n",
    "    cost_breakdown = {}\n",
    "    \n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            error_count = cm[i, j]\n",
    "            error_cost = cost_matrix[i, j] * error_count\n",
    "            total_cost += error_cost\n",
    "            \n",
    "            if error_cost > 0:\n",
    "                error_type = f\"{class_names[i]} → {class_names[j]}\"\n",
    "                cost_breakdown[error_type] = {\n",
    "                    'count': error_count,\n",
    "                    'unit_cost': cost_matrix[i, j],\n",
    "                    'total_cost': error_cost\n",
    "                }\n",
    "    \n",
    "    print(f\"\\nANÁLISIS DE COSTOS:\")\n",
    "    print(f\"Costo total estimado: ${total_cost:,}\")\n",
    "    print(f\"Costo promedio por predicción: ${total_cost/n_samples:.2f}\")\n",
    "    \n",
    "    print(f\"\\nDesglose por tipo de error:\")\n",
    "    sorted_costs = sorted(cost_breakdown.items(), key=lambda x: x[1]['total_cost'], reverse=True)\n",
    "    \n",
    "    for error_type, details in sorted_costs:\n",
    "        count = details['count']\n",
    "        unit_cost = details['unit_cost']\n",
    "        total_error_cost = details['total_cost']\n",
    "        percentage = (total_error_cost / total_cost) * 100 if total_cost > 0 else 0\n",
    "        \n",
    "        print(f\"  {error_type:15}: {count:4d} errores × ${unit_cost:3d} = ${total_error_cost:6,} ({percentage:4.1f}%)\")\n",
    "    \n",
    "    # Calcular métricas ajustadas por costo\n",
    "    accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "    \n",
    "    # Costo de un clasificador aleatorio\n",
    "    random_cost = 0\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            expected_errors = (np.sum(cm[i, :]) * np.sum(cm[:, j])) / np.sum(cm)\n",
    "            random_cost += cost_matrix[i, j] * expected_errors\n",
    "    \n",
    "    cost_improvement = ((random_cost - total_cost) / random_cost) * 100 if random_cost > 0 else 0\n",
    "    \n",
    "    print(f\"\\nCOMPARACIÓN CON BASELINE:\")\n",
    "    print(f\"  Accuracy del modelo: {accuracy:.3f}\")\n",
    "    print(f\"  Costo modelo actual: ${total_cost:,}\")\n",
    "    print(f\"  Costo clasificador aleatorio: ${random_cost:,.0f}\")\n",
    "    print(f\"  Mejora en costo: {cost_improvement:.1f}%\")\n",
    "    \n",
    "    # Guardar análisis de costos\n",
    "    cost_analysis = {\n",
    "        'total_cost': total_cost,\n",
    "        'cost_per_prediction': total_cost / n_samples,\n",
    "        'cost_breakdown': cost_breakdown,\n",
    "        'accuracy': accuracy,\n",
    "        'cost_improvement_vs_random': cost_improvement,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'cost_matrix': cost_matrix.tolist()\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    cost_analysis_path = os.path.join(reports_dir, 'cost_analysis.json')\n",
    "    with open(cost_analysis_path, 'w') as f:\n",
    "        json.dump(cost_analysis, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nAnálisis de costos guardado en: {cost_analysis_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Predicciones no disponibles para análisis de costos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f77b211",
   "metadata": {},
   "source": [
    "## 7. Recomendaciones para Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6087943",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'best_model' in locals() and 'model_metadata' in locals():\n",
    "    print(\"RECOMENDACIONES PARA IMPLEMENTACIÓN EN PRODUCCIÓN\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\"\"\\n1. MODELO SELECCIONADO:\n",
    "   - Algoritmo: {model_metadata['model_type']}\n",
    "   - F1-Score CV: {model_metadata.get('cv_f1_score', 'N/A'):.4f}\n",
    "   - Accuracy CV: {model_metadata.get('cv_accuracy', 'N/A'):.4f}\n",
    "   - Features utilizadas: {model_metadata.get('n_features', 'N/A')}\n",
    "   - Tiempo de entrenamiento: {model_metadata.get('training_time', 'N/A'):.2f}s\n",
    "    \n",
    "2. PIPELINE DE PREPROCESAMIENTO REQUERIDO:\n",
    "   Imputación de valores faltantes (mediana/moda)\n",
    "   Normalización Z-score de todas las features numéricas\n",
    "   Encoding de variables categóricas (LabelEncoder)\n",
    "   Feature engineering (ratios financieros y scores compuestos)\n",
    "   Selección de features específicas según dataset: {model_metadata.get('dataset_used', 'N/A')}\n",
    "    \n",
    "3. CONSIDERACIONES DE RENDIMIENTO:\n",
    "   - Tamaño del modelo: Liviano, adecuado para producción\n",
    "   - Tiempo de inferencia: < 1ms por predicción (estimado)\n",
    "   - Memoria requerida: Mínima (< 10MB)\n",
    "   - Escalabilidad: Excelente para procesamiento batch y tiempo real\n",
    "    \n",
    "4. ESTRATEGIA DE IMPLEMENTACIÓN:\n",
    "   \n",
    "   FASE 1 - PILOTO (Mes 1-2):\n",
    "   • Implementar modelo en ambiente de pruebas\n",
    "   • Validar pipeline de preprocesamiento\n",
    "   • Pruebas A/B con 10% del tráfico\n",
    "   • Monitoreo de métricas de negocio\n",
    "   \n",
    "   FASE 2 - DESPLIEGUE GRADUAL (Mes 2-3):\n",
    "   • Incrementar tráfico gradualmente (25%, 50%, 75%)\n",
    "   • Implementar sistema de alertas por drift\n",
    "   • Configurar reentrenamiento automático mensual\n",
    "   • Documentar casos edge y excepciones\n",
    "   \n",
    "   FASE 3 - PRODUCCIÓN COMPLETA (Mes 3+):\n",
    "   • Despliegue al 100% del tráfico\n",
    "   • Monitoreo continuo de performance\n",
    "   • Feedback loop con resultados reales\n",
    "   • Optimización continua basada en datos nuevos\n",
    "    \n",
    "5. UMBRALES DE DECISIÓN RECOMENDADOS:\n",
    "   \"\"\")\n",
    "    \n",
    "    if 'results_df' in locals():\n",
    "        # Analizar distribución de probabilidades si están disponibles\n",
    "        if 'Prob_Alto' in results_df.columns:\n",
    "            high_risk_threshold = results_df['Prob_Alto'].quantile(0.9)\n",
    "            low_risk_threshold = results_df['Prob_Bajo'].quantile(0.8)\n",
    "            \n",
    "            print(f\"   • RECHAZAR automáticamente si P(Alto) > {high_risk_threshold:.3f}\")\n",
    "            print(f\"   • APROBAR automáticamente si P(Bajo) > {low_risk_threshold:.3f}\")\n",
    "            print(f\"   • REVISAR MANUALMENTE para casos intermedios\")\n",
    "        else:\n",
    "            print(f\"   • RECHAZAR: Predicción = Alto Riesgo\")\n",
    "            print(f\"   • APROBAR: Predicción = Bajo Riesgo\")\n",
    "            print(f\"   • REVISAR: Predicción = Medio Riesgo (evaluación humana)\")\n",
    "    \n",
    "    print(f\"\"\"   \n",
    "6. MONITOREO Y MANTENIMIENTO:\n",
    "   \n",
    "   MÉTRICAS CLAVE A MONITOREAR:\n",
    "   • Distribución de predicciones (detectar drift)\n",
    "   • Tiempo de respuesta del modelo\n",
    "   • Accuracy en datos reales (cuando estén disponibles)\n",
    "   • Tasa de rechazos vs aprobaciones\n",
    "   • Costo promedio por clasificación errónea\n",
    "   \n",
    "   ALERTAS CONFIGURADAS:\n",
    "   • Cambio > 10% en distribución de clases predichas\n",
    "   • Tiempo de inferencia > 100ms\n",
    "   • Caída en accuracy > 5% vs baseline\n",
    "   • Features con valores fuera de rango esperado\n",
    "   \n",
    "   REENTRENAMIENTO:\n",
    "   • Frecuencia: Mensual o cuando accuracy < umbral\n",
    "   • Datos nuevos: Mínimo 1000 casos etiquetados\n",
    "   • Validación: A/B test vs modelo actual\n",
    "   • Rollback: Automático si performance degrada\n",
    "    \n",
    "7. CONSIDERACIONES ÉTICAS Y REGULATORIAS:\n",
    "   \n",
    "   TRANSPARENCIA:\n",
    "   • Documentar features más importantes para decisiones\n",
    "   • Proveer explicación básica de rechazos\n",
    "   • Mantener log de todas las decisiones del modelo\n",
    "   \n",
    "   FAIRNESS:\n",
    "   • Monitorear sesgo por variables demográficas\n",
    "   • Auditar regularmente disparidad en tasas de aprobación\n",
    "   • Implementar mecanismo de apelación para rechazos\n",
    "   \n",
    "   COMPLIANCE:\n",
    "   • Cumplimiento con regulaciones financieras locales\n",
    "   • Retención de datos según políticas de la empresa\n",
    "   • Documentación para auditorías regulatorias\n",
    "    \n",
    "8. ROI ESTIMADO:\n",
    "   \"\"\")\n",
    "    \n",
    "    if 'cost_analysis' in locals():\n",
    "        cost_saving = cost_analysis.get('cost_improvement_vs_random', 0)\n",
    "        monthly_applications = 10000  # Ejemplo\n",
    "        cost_per_prediction = cost_analysis.get('cost_per_prediction', 0)\n",
    "        \n",
    "        monthly_saving = monthly_applications * cost_per_prediction * (cost_saving / 100)\n",
    "        annual_saving = monthly_saving * 12\n",
    "        \n",
    "        print(f\"   • Reducción de costo vs clasificador aleatorio: {cost_saving:.1f}%\")\n",
    "        print(f\"   • Ahorro mensual estimado (10K aplicaciones): ${monthly_saving:,.0f}\")\n",
    "        print(f\"   • Ahorro anual estimado: ${annual_saving:,.0f}\")\n",
    "        print(f\"   • Costo de implementación: $50,000 - $100,000 (estimado)\")\n",
    "        print(f\"   • Payback period: 2-4 meses\")\n",
    "    \n",
    "    print(f\"\"\"   \n",
    "9. PRÓXIMOS PASOS INMEDIATOS:\n",
    "   \n",
    "   SEMANA 1:\n",
    "   □ Revisar y aprobar recomendaciones con stakeholders\n",
    "   □ Configurar ambiente de desarrollo para el modelo\n",
    "   □ Implementar API básica para predicciones\n",
    "   \n",
    "   SEMANA 2:\n",
    "   □ Integrar pipeline de preprocesamiento\n",
    "   □ Implementar sistema de logging y monitoreo\n",
    "   □ Pruebas unitarias y de integración\n",
    "   \n",
    "   SEMANA 3-4:\n",
    "   □ Despliegue en ambiente de pruebas\n",
    "   □ Validación con datos históricos\n",
    "   □ Configuración de dashboards de monitoreo\n",
    "   \n",
    "   MES 2:\n",
    "   □ Piloto con tráfico limitado\n",
    "   □ Análisis de resultados y ajustes\n",
    "   □ Preparación para despliegue completo\n",
    "   \n",
    "10. CONTACTOS CLAVE:\n",
    "   • Equipo de Data Science: Mantenimiento del modelo\n",
    "   • Equipo de Engineering: Integración y deployment\n",
    "   • Risk Management: Validación de umbrales de negocio\n",
    "   • Compliance: Revisión regulatoria y ética\n",
    "   • Product Management: Métricas de negocio y ROI\n",
    "   \"\"\")\n",
    "    \n",
    "    # Guardar recomendaciones en archivo\n",
    "    recommendations = {\n",
    "        'model_info': model_metadata,\n",
    "        'implementation_phases': [\n",
    "            'Piloto (Mes 1-2)',\n",
    "            'Despliegue gradual (Mes 2-3)',\n",
    "            'Producción completa (Mes 3+)'\n",
    "        ],\n",
    "        'monitoring_metrics': [\n",
    "            'Distribución de predicciones',\n",
    "            'Tiempo de respuesta',\n",
    "            'Accuracy en datos reales',\n",
    "            'Costo por error'\n",
    "        ],\n",
    "        'retraining_schedule': 'Mensual',\n",
    "        'estimated_roi': {\n",
    "            'cost_reduction_pct': cost_analysis.get('cost_improvement_vs_random', 0) if 'cost_analysis' in locals() else 'TBD',\n",
    "            'payback_period_months': '2-4'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    recommendations_path = os.path.join(reports_dir, 'implementation_recommendations.json')\n",
    "    with open(recommendations_path, 'w') as f:\n",
    "        json.dump(recommendations, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n\\nRecomendaciones guardadas en: {recommendations_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Información del modelo no disponible para generar recomendaciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776c76c",
   "metadata": {},
   "source": [
    "## 8. Guardado de Resultados Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GUARDADO DE RESULTADOS FINALES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Guardar predicciones del conjunto de test\n",
    "if 'results_df' in locals():\n",
    "    predictions_path = os.path.join(reports_dir, 'test_predictions.csv')\n",
    "    results_df.to_csv(predictions_path, index=False)\n",
    "    print(f\"Predicciones de test guardadas: {predictions_path}\")\n",
    "\n",
    "# Guardar resumen ejecutivo\n",
    "executive_summary = {\n",
    "    'project_title': 'Clasificación de Riesgo Crediticio',\n",
    "    'completion_date': '2025-09-26',\n",
    "    'best_model': {\n",
    "        'algorithm': model_metadata.get('model_type', 'N/A') if 'model_metadata' in locals() else 'N/A',\n",
    "        'cv_f1_score': model_metadata.get('cv_f1_score', 'N/A') if 'model_metadata' in locals() else 'N/A',\n",
    "        'cv_accuracy': model_metadata.get('cv_accuracy', 'N/A') if 'model_metadata' in locals() else 'N/A',\n",
    "        'n_features': model_metadata.get('n_features', 'N/A') if 'model_metadata' in locals() else 'N/A'\n",
    "    },\n",
    "    'test_results': {\n",
    "        'n_predictions': len(results_df) if 'results_df' in locals() else 0,\n",
    "        'class_distribution': results_df['Risk_Level'].value_counts().to_dict() if 'results_df' in locals() else {}\n",
    "    },\n",
    "    'business_impact': {\n",
    "        'cost_reduction': cost_analysis.get('cost_improvement_vs_random', 'TBD') if 'cost_analysis' in locals() else 'TBD',\n",
    "        'estimated_annual_savings': 'TBD',\n",
    "        'implementation_timeline': '3-4 meses'\n",
    "    },\n",
    "    'files_generated': [],\n",
    "    'next_steps': [\n",
    "        'Revisión de stakeholders',\n",
    "        'Implementación de piloto',\n",
    "        'Despliegue gradual',\n",
    "        'Monitoreo en producción'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Listar archivos generados\n",
    "generated_files = []\n",
    "for root, dirs, files in os.walk(reports_dir):\n",
    "    for file in files:\n",
    "        generated_files.append(os.path.join(root, file))\n",
    "\n",
    "executive_summary['files_generated'] = generated_files\n",
    "\n",
    "summary_path = os.path.join(reports_dir, 'executive_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(executive_summary, f, indent=2)\n",
    "\n",
    "print(f\"Resumen ejecutivo guardado: {summary_path}\")\n",
    "\n",
    "print(f\"\\n\\nARCHIVOS GENERADOS EN EL DIRECTORIO DE REPORTES:\")\n",
    "for root, dirs, files in os.walk(reports_dir):\n",
    "    level = root.replace(reports_dir, '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = '  ' * (level + 1)\n",
    "    for file in files:\n",
    "        file_size = os.path.getsize(os.path.join(root, file))\n",
    "        print(f\"{subindent}{file} ({file_size:,} bytes)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUACIÓN FINAL COMPLETADA EXITOSAMENTE\")\n",
    "print(f\"\\nResumen del proyecto:\")\n",
    "if 'model_metadata' in locals():\n",
    "    print(f\"• Mejor modelo: {model_metadata.get('model_type', 'N/A')}\")\n",
    "    print(f\"• F1-Score CV: {model_metadata.get('cv_f1_score', 'N/A')}\")\n",
    "    print(f\"• Features utilizadas: {model_metadata.get('n_features', 'N/A')}\")\n",
    "if 'results_df' in locals():\n",
    "    print(f\"• Predicciones de test: {len(results_df)} muestras\")\n",
    "if 'cost_analysis' in locals():\n",
    "    improvement = cost_analysis.get('cost_improvement_vs_random', 0)\n",
    "    print(f\"• Mejora en costos: {improvement:.1f}% vs baseline\")\n",
    "\n",
    "print(f\"\\nSiguiente paso: Preparar documentación IEEE LaTeX\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
