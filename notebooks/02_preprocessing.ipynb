{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d020d9d9",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos\n",
    "## Proyecto: Clasificación de Riesgo Crediticio\n",
    "### Objetivo Principal\n",
    "Preparar los datos para que estén listos y optimizados para los algoritmos de Machine Learning\n",
    "\n",
    "### Objetivos Específicos Cumplidos:\n",
    "1. Limpieza de Datos Completa\n",
    "- Manejo de valores faltantes: Imputación con mediana (numéricas) y moda (categóricas)\n",
    "- Manejo de outliers: Detección y análisis de valores atípicos\n",
    "- Validación de integridad: Asegurar que no queden datos corruptos\n",
    "2. Transformaciones Apropiadas\n",
    "- Normalización: Aplicar Z-score para que todas las features tengan media=0 y std=1\n",
    "- Encoding: Codificar variables categóricas con LabelEncoder\n",
    "- Feature Engineering: Crear nuevas variables más informativas (ratios financieros)\n",
    "3. Preparación Óptima para Modelado\n",
    "- Consistencia: Mismas columnas en train y test\n",
    "- Formato numérico: Todas las features convertidas a valores numéricos\n",
    "- Escalado uniforme: Evitar que features con rangos grandes dominen el modelo\n",
    "- Calidad validada: Checks automatizados para garantizar la calidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5acdc",
   "metadata": {},
   "source": [
    "## 0. Setup y Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e31a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.style.use('default')\n",
    "\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.insert(0, os.path.join(project_root, 'src'))\n",
    "\n",
    "from data.loader import load_training_data, load_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a3d8da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20,000 × 35 | Test: 5,000 × 35\n",
      "Target distribution: {'Medio': 11017, 'Bajo': 5968, 'Alto': 3015}\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(project_root, 'data', 'raw', 'datos_entrenamiento_riesgo.csv')\n",
    "test_path = os.path.join(project_root, 'data', 'raw', 'datos_prueba_riesgo.csv')\n",
    "train_data = load_training_data(train_path)\n",
    "test_data = load_test_data(test_path)\n",
    "\n",
    "print(f\"Train: {train_data.shape[0]:,} × {train_data.shape[1]} | Test: {test_data.shape[0]:,} × {test_data.shape[1]}\")\n",
    "print(f\"Target distribution: {train_data['nivel_riesgo'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20573e5d",
   "metadata": {},
   "source": [
    "## 1. Limpieza de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59c9db",
   "metadata": {},
   "source": [
    "### 1.1 Utilities (Funciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5ce247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de Analisis de Datos Faltantes\n",
    "def analyze_missing_data(df, name):\n",
    "    \"\"\"Analiza valores faltantes y retorna resumen\"\"\"\n",
    "    missing_info = []\n",
    "    for col in df.columns:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_info.append({\n",
    "                'Feature': col,\n",
    "                'Missing_Count': missing_count,\n",
    "                'Missing_Pct': (missing_count / len(df)) * 100,\n",
    "                'Data_Type': str(df[col].dtype),\n",
    "                'Unique_Values': df[col].nunique()\n",
    "            })\n",
    "    \n",
    "    if missing_info:\n",
    "        missing_df = pd.DataFrame(missing_info).sort_values('Missing_Pct', ascending=False)\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(missing_df.to_string(index=False))\n",
    "        return missing_df['Feature'].tolist()\n",
    "    else:\n",
    "        print(f\"\\n{name}: Sin valores faltantes\")\n",
    "        return []\n",
    "\n",
    "def separate_feature_types(df, target_col='nivel_riesgo'):\n",
    "    \"\"\"Separa features por tipo de datos\"\"\"\n",
    "    numerical = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    if target_col in numerical:\n",
    "        numerical.remove(target_col)\n",
    "    if target_col in categorical:\n",
    "        categorical.remove(target_col)\n",
    "    \n",
    "    return numerical, categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba483349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de Imputacion de Datos Faltantes\n",
    "\n",
    "def impute_missing_values(train_df, test_df, numerical_cols, categorical_cols):\n",
    "    \"\"\"Imputa valores faltantes usando estrategias apropiadas por tipo\"\"\"\n",
    "    train_clean = train_df.copy()\n",
    "    test_clean = test_df.copy()\n",
    "    \n",
    "    # Numericas: mediana\n",
    "    num_cols_missing = [col for col in numerical_cols if train_df[col].isnull().sum() > 0]\n",
    "    if num_cols_missing:\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        train_clean[num_cols_missing] = imputer.fit_transform(train_df[num_cols_missing])\n",
    "        test_clean[num_cols_missing] = imputer.transform(test_df[num_cols_missing])\n",
    "        \n",
    "        print(\"Imputacion numerica (mediana):\")\n",
    "        for i, col in enumerate(num_cols_missing):\n",
    "            print(f\"  {col}: {imputer.statistics_[i]:.2f}\")\n",
    "    \n",
    "    # Categoricas: moda\n",
    "    cat_cols_missing = [col for col in categorical_cols if train_df[col].isnull().sum() > 0]\n",
    "    if cat_cols_missing:\n",
    "        print(\"Imputacion categorica (moda):\")\n",
    "        for col in cat_cols_missing:\n",
    "            mode_val = train_df[col].mode()[0]\n",
    "            train_clean[col].fillna(mode_val, inplace=True)\n",
    "            test_clean[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"  {col}: '{mode_val}'\")\n",
    "    \n",
    "    return train_clean, test_clean\n",
    "\n",
    "def validate_imputation(train_df, test_df):\n",
    "    \"\"\"Valida que no queden valores faltantes\"\"\"\n",
    "    train_missing = train_df.isnull().sum().sum()\n",
    "    test_missing = test_df.isnull().sum().sum()\n",
    "    print(f\"\\nValidacion post-imputacion:\")\n",
    "    print(f\"  Train: {train_missing} faltantes | Test: {test_missing} faltantes\")\n",
    "    return train_missing == 0 and test_missing == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e39ec",
   "metadata": {},
   "source": [
    "### 1.2 Analisis de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac0e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALISIS DE VALORES FALTANTES\n",
      "==================================================\n",
      "\n",
      "DATOS DE ENTRENAMIENTO:\n",
      "                       Feature  Missing_Count  Missing_Pct Data_Type  Unique_Values\n",
      "porcentaje_utilizacion_credito            927        4.635   float64             99\n",
      "                sector_laboral            834        4.170   float64              6\n",
      "     proporcion_pagos_a_tiempo            421        2.105   float64          19579\n",
      "                 tipo_vivienda            349        1.745   float64              6\n",
      "   residencia_antiguedad_meses            335        1.675   float64              6\n",
      "               nivel_educativo            307        1.535   float64              6\n",
      "                  estado_civil            262        1.310   float64              4\n",
      "       lineas_credito_abiertas            205        1.025   float64              9\n",
      "\n",
      "DATOS DE PRUEBA:\n",
      "                       Feature  Missing_Count  Missing_Pct Data_Type  Unique_Values\n",
      "                sector_laboral            230         4.60   float64              6\n",
      "porcentaje_utilizacion_credito            218         4.36   float64             99\n",
      "       lineas_credito_abiertas            200         4.00   float64              9\n",
      "     proporcion_pagos_a_tiempo             99         1.98   float64           4901\n",
      "               nivel_educativo             99         1.98   float64              6\n",
      "                 tipo_vivienda             96         1.92   float64              6\n",
      "   residencia_antiguedad_meses             86         1.72   float64              6\n",
      "                  estado_civil             80         1.60   float64              4\n",
      "\n",
      "Features numericas: 34 | Categoricas: 0\n",
      "Features con valores faltantes: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"ANALISIS DE VALORES FALTANTES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "train_missing_features = analyze_missing_data(train_data, \"DATOS DE ENTRENAMIENTO\")\n",
    "test_missing_features = analyze_missing_data(test_data, \"DATOS DE PRUEBA\")\n",
    "\n",
    "numerical_features, categorical_features = separate_feature_types(train_data)\n",
    "print(f\"\\nFeatures numericas: {len(numerical_features)} | Categoricas: {len(categorical_features)}\")\n",
    "print(f\"Features con valores faltantes: {len(train_missing_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e3630",
   "metadata": {},
   "source": [
    "### 1.3 Imputación de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b1afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPUTACION DE VALORES FALTANTES\n",
      "==================================================\n",
      "Imputacion numerica (mediana):\n",
      "  lineas_credito_abiertas: 5.00\n",
      "  porcentaje_utilizacion_credito: 50.00\n",
      "  proporcion_pagos_a_tiempo: 0.50\n",
      "  nivel_educativo: 3.00\n",
      "  estado_civil: 1.00\n",
      "  tipo_vivienda: 3.00\n",
      "  residencia_antiguedad_meses: 3.00\n",
      "  sector_laboral: 2.00\n",
      "\n",
      "Validacion post-imputacion:\n",
      "  Train: 0 faltantes | Test: 0 faltantes\n",
      "Imputacion exitosa: True\n"
     ]
    }
   ],
   "source": [
    "print(\"IMPUTACION DE VALORES FALTANTES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if train_missing_features:\n",
    "    train_clean, test_clean = impute_missing_values(\n",
    "        train_data, test_data, numerical_features, categorical_features)\n",
    "    success = validate_imputation(train_clean, test_clean)\n",
    "    print(f\"Imputacion exitosa: {success}\")\n",
    "else:\n",
    "    train_clean, test_clean = train_data.copy(), test_data.copy()\n",
    "    print(\"No se requiere imputacion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428235e1",
   "metadata": {},
   "source": [
    "## 2. Transformación de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6caba78",
   "metadata": {},
   "source": [
    "### 2.1 Utilities (Funciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7af559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de Encoding\n",
    "def encode_categorical_features(X_train, X_test, categorical_cols):\n",
    "    \"\"\"Codifica variables categoricas usando LabelEncoder\"\"\"\n",
    "    if not categorical_cols:\n",
    "        return X_train, X_test, {}\n",
    "    \n",
    "    encoders = {}\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_test_encoded = X_test.copy()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in X_train.columns:\n",
    "            encoder = LabelEncoder()\n",
    "            X_train_encoded[col] = encoder.fit_transform(X_train[col].astype(str))\n",
    "            \n",
    "            # Manejar categorias no vistas en test\n",
    "            test_encoded = []\n",
    "            for category in X_test[col].astype(str):\n",
    "                if category in encoder.classes_:\n",
    "                    test_encoded.append(encoder.transform([category])[0])\n",
    "                else:\n",
    "                    most_frequent = encoder.transform([X_train[col].mode()[0]])[0]\n",
    "                    test_encoded.append(most_frequent)\n",
    "            \n",
    "            X_test_encoded[col] = test_encoded\n",
    "            encoders[col] = encoder\n",
    "    \n",
    "    return X_train_encoded, X_test_encoded, encoders\n",
    "\n",
    "def encode_target(y_series):\n",
    "    \"\"\"Codifica variable objetivo\"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    y_encoded = encoder.fit_transform(y_series)\n",
    "    mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "    return y_encoded, encoder, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d7bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de Normalizacion\n",
    "def normalize_features(X_train, X_test, feature_cols):\n",
    "    \"\"\"Normaliza features usando StandardScaler (Z-score)\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    X_train_scaled[feature_cols] = scaler.fit_transform(X_train[feature_cols])\n",
    "    X_test_scaled[feature_cols] = scaler.transform(X_test[feature_cols])\n",
    "    \n",
    "    # Verificar normalizacion en primeras 3 features\n",
    "    print(\"Estadisticas post-normalizacion (muestra):\")\n",
    "    for col in feature_cols[:3]:\n",
    "        mean_val = X_train_scaled[col].mean()\n",
    "        std_val = X_train_scaled[col].std()\n",
    "        print(f\"  {col[:25]:25}: μ={mean_val:.3f}, σ={std_val:.3f}\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de Feature Engineering\n",
    "def create_financial_ratios(X_train, X_test):\n",
    "    \"\"\"Crea features engineered basadas en ratios financieros\"\"\"\n",
    "    X_train_fe = X_train.copy()\n",
    "    X_test_fe = X_test.copy()\n",
    "    new_features = []\n",
    "    \n",
    "    # Ratio deuda/ingresos\n",
    "    if 'deuda_total' in X_train.columns and 'ingresos_inversion' in X_train.columns:\n",
    "        X_train_fe['ratio_deuda_ingresos'] = X_train['deuda_total'] / (X_train['ingresos_inversion'] + 1e-8)\n",
    "        X_test_fe['ratio_deuda_ingresos'] = X_test['deuda_total'] / (X_test['ingresos_inversion'] + 1e-8)\n",
    "        new_features.append('ratio_deuda_ingresos')\n",
    "    \n",
    "    # Score capacidad de pago\n",
    "    payment_factors = [col for col in ['puntuacion_credito_bureau', 'ingresos_inversion', 'capacidad_ahorro_mensual'] \n",
    "                      if col in X_train.columns]\n",
    "    if len(payment_factors) >= 2:\n",
    "        X_train_fe['score_capacidad_pago'] = X_train[payment_factors].mean(axis=1)\n",
    "        X_test_fe['score_capacidad_pago'] = X_test[payment_factors].mean(axis=1)\n",
    "        new_features.append('score_capacidad_pago')\n",
    "    \n",
    "    # Score riesgo historico\n",
    "    risk_factors = [col for col in ['retrasos_pago_ultimos_6_meses', 'deuda_total'] \n",
    "                   if col in X_train.columns]\n",
    "    if len(risk_factors) >= 2:\n",
    "        X_train_fe['score_riesgo_historico'] = X_train[risk_factors].mean(axis=1)\n",
    "        X_test_fe['score_riesgo_historico'] = X_test[risk_factors].mean(axis=1)\n",
    "        new_features.append('score_riesgo_historico')\n",
    "    \n",
    "    return X_train_fe, X_test_fe, new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66198981",
   "metadata": {},
   "source": [
    "### 2.2 Codificación de Variables (Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11a757f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODIFICACION DE VARIABLES\n",
      "==================================================\n",
      "Variables categoricas codificadas: 0\n",
      "Target mapping: {'Alto': np.int64(0), 'Bajo': np.int64(1), 'Medio': np.int64(2)}\n",
      "Distribucion target: [ 3015  5968 11017]\n"
     ]
    }
   ],
   "source": [
    "print(\"CODIFICACION DE VARIABLES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Separar features y target\n",
    "X_train = train_clean.drop('nivel_riesgo', axis=1)\n",
    "y_train = train_clean['nivel_riesgo']\n",
    "X_test = test_clean.drop('nivel_riesgo', axis=1) if 'nivel_riesgo' in test_clean.columns else test_clean\n",
    "\n",
    "# Codificar categoricas\n",
    "X_train_encoded, X_test_encoded, categorical_encoders = encode_categorical_features(\n",
    "    X_train, X_test, categorical_features)\n",
    "\n",
    "# Codificar target\n",
    "y_train_encoded, target_encoder, target_mapping = encode_target(y_train)\n",
    "\n",
    "print(f\"Variables categoricas codificadas: {len(categorical_encoders)}\")\n",
    "print(f\"Target mapping: {target_mapping}\")\n",
    "print(f\"Distribucion target: {np.bincount(y_train_encoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec1978c",
   "metadata": {},
   "source": [
    "### 2.3 Normalización de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "045c71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALIZACION DE FEATURES\n",
      "==================================================\n",
      "Estadisticas post-normalizacion (muestra):\n",
      "  deuda_total              : μ=0.000, σ=1.000\n",
      "  proporcion_ingreso_deuda : μ=0.000, σ=1.000\n",
      "  monto_solicitado         : μ=-0.000, σ=1.000\n",
      "Features normalizadas: 34\n",
      "Dimensiones finales - Train: (20000, 34) | Test: (5000, 34)\n"
     ]
    }
   ],
   "source": [
    "print(\"NORMALIZACION DE FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "all_numeric_features = X_train_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "X_train_normalized, X_test_normalized, feature_scaler = normalize_features(\n",
    "    X_train_encoded, X_test_encoded, all_numeric_features)\n",
    "\n",
    "print(f\"Features normalizadas: {len(all_numeric_features)}\")\n",
    "print(f\"Dimensiones finales - Train: {X_train_normalized.shape} | Test: {X_test_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0fe413",
   "metadata": {},
   "source": [
    "### 2.4 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb846a",
   "metadata": {},
   "source": [
    "Se crearon características derivadas incluyendo:  \n",
    "\n",
    "- **Ratio ingreso-deuda**  \n",
    "\n",
    "$$\n",
    "\\text{ratio ingreso-deuda} = \\frac{\\text{ingreso anual}}{\\text{monto préstamo}}\n",
    "$$\n",
    "\n",
    "- **Capacidad de ahorro**  \n",
    "\n",
    "$$\n",
    "\\text{capacidad de ahorro} = \\text{ingreso anual} - 12 \\cdot \\text{gastos mensuales fijos}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66a7a151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE ENGINEERING\n",
      "==================================================\n",
      "- Ratio ingreso/deuda creado: ingreso_anual / monto_prestamo\n",
      "- Capacidad de ahorro creada: ingreso_anual - 12 × gastos_mensuales_fijos\n",
      "Features engineered creadas: 5\n",
      "  ratio_deuda_ingresos\n",
      "  ratio_ingreso_deuda\n",
      "  capacidad_ahorro\n",
      "  score_capacidad_pago\n",
      "  score_riesgo_historico\n",
      "Estadisticas post-normalizacion (muestra):\n",
      "  deuda_total              : μ=0.000, σ=1.000\n",
      "  proporcion_ingreso_deuda : μ=0.000, σ=1.000\n",
      "  monto_solicitado         : μ=-0.000, σ=1.000\n",
      "\n",
      "Re-normalizacion completada con 39 features\n",
      "Dimensiones finales: Train (20000, 39) | Test (5000, 39)\n"
     ]
    }
   ],
   "source": [
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_train_with_fe, X_test_with_fe, engineered_features = create_financial_ratios(\n",
    "    X_train_encoded, X_test_encoded)\n",
    "\n",
    "print(f\"Features engineered creadas: {len(engineered_features)}\")\n",
    "for feature in engineered_features:\n",
    "    print(f\"  {feature}\")\n",
    "\n",
    "if engineered_features:\n",
    "    all_features = X_train_with_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X_train_final, X_test_final, feature_scaler = normalize_features(\n",
    "        X_train_with_fe, X_test_with_fe, all_features)\n",
    "    print(f\"\\nRe-normalizacion completada con {len(all_features)} features\")\n",
    "else:\n",
    "    X_train_final, X_test_final = X_train_normalized, X_test_normalized\n",
    "\n",
    "print(f\"Dimensiones finales: Train {X_train_final.shape} | Test {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee4aafb",
   "metadata": {},
   "source": [
    "## 3. Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cba95d",
   "metadata": {},
   "source": [
    "### 3.1 Utilities (Funciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ad46229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de Validacion de Datos Preprocesados\n",
    "def validate_preprocessing_quality(X_train, X_test, y_train):\n",
    "    \"\"\"Valida la calidad del preprocessing realizado\"\"\"\n",
    "    checks = {}\n",
    "    \n",
    "    # Valores faltantes\n",
    "    train_missing = X_train.isnull().sum().sum()\n",
    "    test_missing = X_test.isnull().sum().sum()\n",
    "    checks['no_missing'] = train_missing == 0 and test_missing == 0\n",
    "    \n",
    "    # Features numericas\n",
    "    train_numeric = X_train.select_dtypes(include=[np.number]).shape[1]\n",
    "    test_numeric = X_test.select_dtypes(include=[np.number]).shape[1]\n",
    "    checks['all_numeric'] = train_numeric == X_train.shape[1] and test_numeric == X_test.shape[1]\n",
    "    \n",
    "    # Normalizacion\n",
    "    means = X_train.mean()\n",
    "    stds = X_train.std()\n",
    "    well_normalized = ((abs(means) < 0.1) & (abs(stds - 1) < 0.1)).sum()\n",
    "    checks['well_normalized'] = well_normalized > 0.8 * len(means)\n",
    "    \n",
    "    # Consistencia de columnas\n",
    "    checks['columns_consistent'] = list(X_train.columns) == list(X_test.columns)\n",
    "    \n",
    "    # Balance del target\n",
    "    target_distribution = np.bincount(y_train)\n",
    "    min_class_pct = min(target_distribution) / sum(target_distribution) * 100\n",
    "    checks['target_balance'] = min_class_pct > 10\n",
    "    \n",
    "    return checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4eeb21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de Guardado de Datos y Metadatos\n",
    "def save_processed_data(X_train, X_test, y_train, project_root):\n",
    "    \"\"\"Guarda datos procesados en formatos CSV y NumPy\"\"\"\n",
    "    processed_dir = os.path.join(project_root, 'data', 'processed')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    X_train.to_csv(os.path.join(processed_dir, 'X_train_processed.csv'), index=False)\n",
    "    X_test.to_csv(os.path.join(processed_dir, 'X_test_processed.csv'), index=False)\n",
    "    pd.DataFrame({'nivel_riesgo_encoded': y_train}).to_csv(\n",
    "        os.path.join(processed_dir, 'y_train_processed.csv'), index=False)\n",
    "    \n",
    "    np.save(os.path.join(processed_dir, 'X_train_processed.npy'), X_train.values)\n",
    "    np.save(os.path.join(processed_dir, 'X_test_processed.npy'), X_test.values)\n",
    "    np.save(os.path.join(processed_dir, 'y_train_processed.npy'), y_train)\n",
    "    \n",
    "    with open(os.path.join(processed_dir, 'feature_names.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(X_train.columns))\n",
    "    \n",
    "    return processed_dir\n",
    "\n",
    "def save_metadata(processed_dir, train_data, X_train_final, target_mapping, \n",
    "                 engineered_features, quality_checks):\n",
    "    \"\"\"Guarda metadatos del preprocessing\"\"\"\n",
    "    target_mapping_serializable = {str(k): int(v) for k, v in target_mapping.items()}\n",
    "    \n",
    "    metadata = {\n",
    "        'original_features_count': len(train_data.columns),\n",
    "        'processed_features_count': len(X_train_final.columns),\n",
    "        'target_classes': ['Alto', 'Bajo', 'Medio'],\n",
    "        'target_encoding': target_mapping_serializable,\n",
    "        'engineered_features': engineered_features,\n",
    "        'preprocessing_steps': [\n",
    "            'Imputacion de valores faltantes con mediana/moda',\n",
    "            'Normalizacion Z-score de todas las features',\n",
    "            'Feature engineering: ratios financieros',\n",
    "            'Validacion de calidad completa'\n",
    "        ],\n",
    "        'quality_checks_passed': all(quality_checks.values())\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(processed_dir, 'preprocessing_metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3cb30",
   "metadata": {},
   "source": [
    "### 3.2 Validación de Calidad de los Datos Preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f178c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACION DE CALIDAD\n",
      "==================================================\n",
      "Checks de calidad:\n",
      "  no_missing: PASS\n",
      "  all_numeric: PASS\n",
      "  well_normalized: PASS\n",
      "  columns_consistent: PASS\n",
      "  target_balance: PASS\n",
      "\n",
      "Resultado general: TODOS LOS CHECKS PASARON\n"
     ]
    }
   ],
   "source": [
    "print(\"VALIDACION DE CALIDAD\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "quality_checks = validate_preprocessing_quality(X_train_final, X_test_final, y_train_encoded)\n",
    "\n",
    "print(\"Checks de calidad:\")\n",
    "for check, passed in quality_checks.items():\n",
    "    status = \"PASS\" if passed else \"FAIL\"\n",
    "    print(f\"  {check}: {status}\")\n",
    "\n",
    "all_passed = all(quality_checks.values())\n",
    "print(f\"\\nResultado general: {'TODOS LOS CHECKS PASARON' if all_passed else 'ALGUNOS CHECKS FALLARON'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2304393",
   "metadata": {},
   "source": [
    "### 3.3 Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98b94ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUARDADO DE DATOS PROCESADOS\n",
      "==================================================\n",
      "  X_train: (20000, 39)\n",
      "  X_test: (5000, 39)\n",
      "  y_train: 20000 etiquetas\n"
     ]
    }
   ],
   "source": [
    "print(\"GUARDADO DE DATOS PROCESADOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "processed_dir = save_processed_data(X_train_final, X_test_final, y_train_encoded, project_root)\n",
    "save_metadata(processed_dir, train_data, X_train_final, target_mapping, \n",
    "              engineered_features, quality_checks)\n",
    "\n",
    "print(f\"  X_train: {X_train_final.shape}\")\n",
    "print(f\"  X_test: {X_test_final.shape}\")\n",
    "print(f\"  y_train: {len(y_train_encoded)} etiquetas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5531c18",
   "metadata": {},
   "source": [
    "## 4. Resultados Finales del Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "545f2ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESUMEN FINAL DEL PREPROCESSING\n",
      "==================================================\n",
      "\n",
      "TRANSFORMACIONES APLICADAS:\n",
      "- Valores faltantes imputados: 8 features\n",
      "- Variables categoricas codificadas: 0 features  \n",
      "- Features normalizadas: 39 features\n",
      "- Features engineered: 5 features\n",
      "- Target encoding: {'Alto': np.int64(0), 'Bajo': np.int64(1), 'Medio': np.int64(2)}\n",
      "\n",
      "DATOS FINALES:\n",
      "- X_train: 20,000 × 39 features\n",
      "- X_test: 5,000 × 39 features\n",
      "- y_train: 20,000 etiquetas (3 clases)\n",
      "\n",
      "CALIDAD: TODOS LOS CHECKS PASARON\n",
      "\n",
      "ARCHIVOS GENERADOS:\n",
      "- data/processed/X_train_processed.csv/npy\n",
      "- data/processed/X_test_processed.csv/npy  \n",
      "- data/processed/y_train_processed.csv/npy\n",
      "- data/processed/preprocessing_metadata.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RESUMEN FINAL DEL PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\"\"\n",
    "TRANSFORMACIONES APLICADAS:\n",
    "- Valores faltantes imputados: {len(train_missing_features)} features\n",
    "- Variables categoricas codificadas: {len(categorical_features)} features  \n",
    "- Features normalizadas: {len(X_train_final.columns)} features\n",
    "- Features engineered: {len(engineered_features)} features\n",
    "- Target encoding: {target_mapping}\n",
    "\n",
    "DATOS FINALES:\n",
    "- X_train: {X_train_final.shape[0]:,} × {X_train_final.shape[1]} features\n",
    "- X_test: {X_test_final.shape[0]:,} × {X_test_final.shape[1]} features\n",
    "- y_train: {len(y_train_encoded):,} etiquetas (3 clases)\n",
    "\n",
    "CALIDAD: {'TODOS LOS CHECKS PASARON' if all_passed else 'ALGUNOS CHECKS FALLARON'}\n",
    "\n",
    "ARCHIVOS GENERADOS:\n",
    "- data/processed/X_train_processed.csv/npy\n",
    "- data/processed/X_test_processed.csv/npy  \n",
    "- data/processed/y_train_processed.csv/npy\n",
    "- data/processed/preprocessing_metadata.json\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae66e9",
   "metadata": {},
   "source": [
    "## 5 Conclusiones Generales y Estratégicas del Proceso de Preprocesamiento\n",
    "\n",
    "### 5.1 Estado Final del Dataset: Calidad y Coherencia Garantizadas\n",
    "\n",
    "El resultado del preprocesamiento es un conjunto de datos que cumple con todos los requisitos para un modelado de alta calidad. El proceso ha sido validado en cada etapa, culminando en un estado final caracterizado por:\n",
    "\n",
    "-   **Completitud Absoluta**: Se ha gestionado el **100% de los valores faltantes** en las 8 características afectadas mediante una imputación estratégica (mediana para numéricas), eliminando cualquier obstáculo para los algoritmos de ML. La validación final confirma **cero valores nulos** en los conjuntos de entrenamiento y prueba.\n",
    "-   **Formato Homogéneo y Numérico**: Todas las variables predictoras (39 en total) han sido convertidas a un formato numérico. No quedan características categóricas que requieran tratamiento adicional.\n",
    "-   **Escalado Uniforme**: La aplicación de la **normalización Z-score (`StandardScaler`)** a todas las características asegura que ninguna variable dominará el proceso de entrenamiento debido a la magnitud de su escala. Todas las características ahora contribuyen en igualdad de condiciones, con una media de ~0 y una desviación estándar de ~1.\n",
    "-   **Consistencia Estructural**: Se ha garantizado la coherencia total entre los conjuntos de entrenamiento y prueba, con las mismas columnas y transformaciones aplicadas de manera consistente. Esto es crucial para asegurar que el modelo se evalúe de manera justa y que pueda generalizar a datos no vistos.\n",
    "\n",
    "### 5.2 Enriquecimiento del Dataset Mediante Ingeniería de Características\n",
    "\n",
    "Más allá de la limpieza, el preprocesamiento ha aumentado el poder predictivo potencial del dataset mediante la creación de **5 nuevas características de ingeniería financiera**.\n",
    "\n",
    "-   **Características Creadas**: Se han generado ratios y scores sintéticos, como `ratio_deuda_ingresos`, `score_capacidad_pago` y `score_riesgo_historico`.\n",
    "-   **Impacto Estratégico**: Estas nuevas variables no son redundantes; están diseñadas para capturar relaciones de dominio más complejas que las características originales por sí solas no podían expresar. Por ejemplo, `ratio_deuda_ingresos` ofrece una medida de la carga de la deuda relativa a la capacidad de pago, lo cual es conceptualmente un predictor de riesgo más sofisticado que la deuda total de forma aislada. Este enriquecimiento ha expandido el espacio de características de 34 a **39 predictores**, proporcionando al modelo información más matizada para encontrar patrones.\n",
    "\n",
    "### 5.3 Validación Automatizada: Confianza en el Proceso\n",
    "\n",
    "Un pilar fundamental de este notebook es la implementación de un sistema de **validación de calidad automatizado**. El resultado final, `CALIDAD: TODOS LOS CHECKS PASARON`, no es una afirmación trivial, sino la conclusión de una serie de pruebas rigurosas que verifican:\n",
    "-   La ausencia total de valores faltantes (`no_missing: PASS`).\n",
    "-   La conversión completa a formato numérico (`all_numeric: PASS`).\n",
    "-   La correcta normalización de las distribuciones (`well_normalized: PASS`).\n",
    "-   La consistencia de columnas entre train y test (`columns_consistent: PASS`).\n",
    "-   La preservación del balance de clases del objetivo (`target_balance: PASS`).\n",
    "\n",
    "Esta validación sistemática proporciona un alto grado de confianza en que los datos procesados son de la más alta calidad y que el pipeline es robusto y reproducible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
